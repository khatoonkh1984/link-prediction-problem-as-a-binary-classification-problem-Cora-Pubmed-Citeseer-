{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab312d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affc6a1c",
   "metadata": {},
   "source": [
    "##  A library for data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf2d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "from torch.utils.data import IterableDataset\n",
    "import warnings\n",
    "\n",
    "\n",
    "import datatable as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "from torch_geometric.seed import seed_everything as th_seed\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "from utils.utils import set_dirs, update_config_with_model_dims\n",
    "from utils.model import NESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bb42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell there some class and function for loading dataset and preprocessing\n",
    "\n",
    "class GraphLoader:\n",
    "    \"\"\"\n",
    "    Data loader class for graph data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], dataset_name: str, kwargs: Dict[str, Any] = {}) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        kwargs : Dict[str, Any], optional\n",
    "            Dictionary for additional parameters if needed, by default {}.\n",
    "        \"\"\"\n",
    "        # Get config\n",
    "        self.config = config\n",
    "        # Set the seed\n",
    "        th_seed(config[\"seed\"])\n",
    "        # Set the paths\n",
    "        paths = config[\"paths\"]\n",
    "        # data > dataset_name\n",
    "        file_path = os.path.join(paths[\"data\"], dataset_name)\n",
    "        # Get the datasets\n",
    "        self.train_data, self.validation_data, self.test_data = self.get_dataset(dataset_name, file_path)        \n",
    "        \n",
    "\n",
    "    def get_dataset(self, dataset_name: str, file_path: str) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Returns the training, validation, and test datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        file_path : str\n",
    "            Path to the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Graph dataset class\n",
    "        graph_dataset = GraphDataset(self.config, datadir=file_path, dataset_name=dataset_name)\n",
    "        \n",
    "        # Load Training, Validation, Test datasets\n",
    "        train_data, val_data, test_data = graph_dataset._load_data()\n",
    "        \n",
    "        # Generate static subgraphs from training set\n",
    "        train_data = self.generate_subgraphs(train_data)\n",
    "  \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    \n",
    "    def generate_subgraphs(self, train_data: Data) -> List[Data]:\n",
    "        \"\"\"\n",
    "        Generates subgraphs from the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : Data\n",
    "            Training data containing the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Data]\n",
    "            List of subgraphs generated from the training data.\n",
    "        \"\"\"\n",
    "        # Initialize list to hold subgraphs\n",
    "        subgraphs = [train_data]\n",
    "\n",
    "        # Check if we are generating subgraphs from the graph. If False, we are in standard GAE mode\n",
    "        if self.config[\"n_subgraphs\"] > 1:\n",
    "                \n",
    "            # Generate subgraphs\n",
    "            for i in range(self.config[\"n_subgraphs\"]):\n",
    "                \n",
    "                # Change random seed\n",
    "                th_seed(i)\n",
    "                \n",
    "                partition = 1.0/(self.config[\"n_subgraphs\"]-i)\n",
    "                \n",
    "                # For the last subgraph, get 95% of the remaining graph. if num_val=1.0, RandomLinkSplit will raise error\n",
    "                if partition == 1.0:\n",
    "                    partition = 0.95\n",
    "                    \n",
    "                random_link_split = T.RandomLinkSplit(num_val=partition, \n",
    "                                                      num_test=0, \n",
    "                                                      is_undirected=True, \n",
    "                                                      split_labels=True, \n",
    "                                                      add_negative_train_samples=False)\n",
    "\n",
    "                # get a subgraph from training data\n",
    "                train_data, train_subgraph, _ = random_link_split(train_data)\n",
    "                \n",
    "                # Make sure that we are using only the nodes within the subgraph by overwriting the edge index \n",
    "                # with positive edge index + positive edge index reversed in direction (to make it undirected)\n",
    "                pos_swapped = train_subgraph.pos_edge_label_index[[1,0],:] \n",
    "                train_subgraph.edge_index = torch.cat((train_subgraph.pos_edge_label_index, pos_swapped), dim=1)\n",
    "                \n",
    "                # Remove negative edge attributes. We want to sample negative samples during training\n",
    "                # Masks are also not needed\n",
    "                if hasattr(train_subgraph, \"neg_edge_label_index\"):\n",
    "                    delattr(train_subgraph, \"neg_edge_label_index\")\n",
    "                    delattr(train_subgraph, \"neg_edge_label\")\n",
    "                    \n",
    "                if hasattr(train_subgraph, \"train_mask\"):\n",
    "                    delattr(train_subgraph, \"train_mask\")\n",
    "                    delattr(train_subgraph, \"val_mask\")\n",
    "                    delattr(train_subgraph, \"test_mask\")\n",
    "\n",
    "\n",
    "                # store the sampled subgraph\n",
    "                subgraphs = [train_subgraph] + subgraphs\n",
    "                       \n",
    "        # Change random seed back to original\n",
    "        th_seed(self.config[\"seed\"])\n",
    "        \n",
    "        # Return all subgraphs and original larger graph\n",
    "        return subgraphs\n",
    "\n",
    "    \n",
    "def get_transform(options):\n",
    "    \"\"\"Splits data to train, validation and test, and moves them to the device\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(options[\"device\"]),\n",
    "        T.RandomLinkSplit(num_val=0.05, \n",
    "                          num_test=0.15, \n",
    "                          is_undirected=True,\n",
    "                          split_labels=True, \n",
    "                          add_negative_train_samples=False),\n",
    "        ])\n",
    "        \n",
    "    return transform\n",
    "\n",
    "\n",
    "class GraphDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for graph data format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], datadir: str, dataset_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        datadir : str\n",
    "            The path to the data directory.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.paths = config[\"paths\"]\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_path = os.path.join(self.paths[\"data\"], 'Planetoid')\n",
    "        self.transform = get_transform(config)\n",
    "\n",
    "        \n",
    "    def _load_data(self) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Loads one of many available datasets and returns features and labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "        if self.dataset_name.lower() in ['cora', 'citeseer', 'pubmed']:\n",
    "            # Get the dataset\n",
    "            dataset = Planetoid(self.data_path, self.dataset_name, split=\"random\", transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in ['chameleon']:\n",
    "            # Get the dataset\n",
    "            dataset = WikipediaNetwork(root=self.data_path, name=self.dataset_name, transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in [\"cornell\", \"texas\", \"wisconsin\"]:\n",
    "            # Get the dataset\n",
    "            dataset = WebKB(root=self.data_path, name=self.dataset_name, transform = self.transform) \n",
    "        else:\n",
    "            print(f\"Given dataset name is not found. Check for typos, or missing condition \")\n",
    "            exit()\n",
    "            \n",
    "        # Data splits\n",
    "        train_data, val_data, test_data = dataset[0]\n",
    "        \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540aa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loads arguments and configuration for GNN-based encoder used in NESS.\n",
    "\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "from argparse import ArgumentParser\n",
    "from os.path import abspath, dirname\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.utils import get_runtime_and_model_config, print_config\n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "    # Initialize parser\n",
    "    parser = ArgumentParser()\n",
    "    # Dataset can be provided via command line\n",
    "    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"cora\")\n",
    "    # Encoder type\n",
    "    parser.add_argument(\"-gnn\", \"--gnn\", type=str, default=\"GNAE\")\n",
    "    # Random seed\n",
    "    parser.add_argument(\"-seed\", \"--seed\", type=int, default=57)\n",
    "    # Whether to use contrastive loss\n",
    "    parser.add_argument(\"-cl\", \"--cl\", type=bool, default=False)\n",
    "    # Whether to add noise to input\n",
    "    parser.add_argument(\"-an\", \"--an\", type=bool, default=True)\n",
    "    # Whether to use GPU.\n",
    "    parser.add_argument(\"-g\", \"--gpu\", dest='gpu', action='store_true', \n",
    "                        help='Used to assign GPU as the device, assuming that GPU is available')\n",
    "    \n",
    "    parser.add_argument(\"-ng\", \"--no_gpu\", dest='gpu', action='store_false', \n",
    "                        help='Used to assign CPU as the device')\n",
    "    parser.set_defaults(gpu=True)\n",
    "    \n",
    "    # GPU device number as in \"cuda:0\". Defaul is 0.\n",
    "    parser.add_argument(\"-dn\", \"--device_number\", type=str, default='0', \n",
    "                        help='Defines which GPU to use. It is 0 by default')\n",
    "    \n",
    "    # Experiment number\n",
    "    parser.add_argument(\"-ex\", \"--experiment\", type=int, default=1)\n",
    "    # Load model saved at specific epoch\n",
    "    parser.add_argument(\"-m\", \"--model_at_epoch\", type=int, default=None)\n",
    "    \n",
    "    # Return parser arguments along with the unknown ones\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_config(args):\n",
    "    # Load runtime config from config folder: ./config/\n",
    "    config = get_runtime_and_model_config(args)\n",
    "    # Define which device to use: GPU or CPU\n",
    "    config[\"device\"] = torch.device('cuda:'+args.device_number if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    # Model at specific epoch\n",
    "    config[\"model_at_epoch\"] = args.model_at_epoch\n",
    "    # Indicate which device is being used\n",
    "    print(f\"Device being used is {config['device']}\")\n",
    "    # Return\n",
    "    return config\n",
    "\n",
    "def print_config_summary(config, args=None):\n",
    "    \"\"\"Prints out summary of options and arguments used\"\"\"\n",
    "    # Summarize config on the screen as a sanity check\n",
    "    print(100 * \"=\")\n",
    "    print(f\"Here is the configuration being used:\\n\")\n",
    "    print_config(config)\n",
    "    print(100 * \"=\")\n",
    "    if args is not None:\n",
    "        print(f\"Arguments being used:\\n\")\n",
    "        print_config(args)\n",
    "        print(100 * \"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f0e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used is cpu\n"
     ]
    }
   ],
   "source": [
    "# Get parser / command line arguments\n",
    "args = get_arguments()\n",
    "# Get configuration file\n",
    "config = get_config(args)\n",
    "\n",
    "\n",
    "# By default, we are using the name of the dataset. This can be customized.\n",
    "config[\"experiment\"] = config[\"dataset\"]\n",
    "\n",
    "# File name to use when saving results as csv. This can be customized\n",
    "config[\"file_name\"] = config[\"experiment\"] + \"_sub\" + str(config[\"n_subgraphs\"]) + '_seed' + str(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b334b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Here is the configuration being used:\n",
      "\n",
      "+-------------------+----------------------------------------------+\n",
      "|     Parameter     |                    Value                     |\n",
      "+===================+==============================================+\n",
      "| Add noise         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Aggregation       | mean                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Batch size        | 128                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Contrastive loss  | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Cosine similarity | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dataset           | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Device            | cpu                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dropout rate      | 0.500                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Encoder type      | GNAE                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Epochs            | 500                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Experiment        | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| File name         | cora_sub4_seed57                             |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Full graph        | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isbatchnorm       | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isdropout         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Learning rate     | 0.010                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Model at epoch    | None                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| N subgraphs       | 4                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Normalize         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Nth epoch         | 10                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P noise           | 0.200                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P norm            | 2                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Paths             | {'data': './data/', 'results': './results/'} |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Patience          | 3                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Scheduler         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Seed              | 57                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Tau               | 0.100                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Validate          | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Z dim             | 32                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "====================================================================================================\n",
      "Arguments being used:\n",
      "\n",
      "+----------------+-------+\n",
      "|   Parameter    | Value |\n",
      "+================+=======+\n",
      "| An             | True  |\n",
      "+----------------+-------+\n",
      "| Cl             | False |\n",
      "+----------------+-------+\n",
      "| Dataset        | cora  |\n",
      "+----------------+-------+\n",
      "| Device number  | 0     |\n",
      "+----------------+-------+\n",
      "| Experiment     | 1     |\n",
      "+----------------+-------+\n",
      "| Gnn            | GNAE  |\n",
      "+----------------+-------+\n",
      "| Gpu            | True  |\n",
      "+----------------+-------+\n",
      "| Model at epoch | None  |\n",
      "+----------------+-------+\n",
      "| Seed           | 57    |\n",
      "+----------------+-------+\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summarize config and arguments on the screen as a sanity check\n",
    "print_config_summary(config, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f86388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these two function for training dataset\n",
    "\n",
    "def train(config: Dict, data_loader: IterableDataset, save_weights: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model using provided configuration and data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "\n",
    "    data_loader : IterableDataset\n",
    "        Pytorch data loader used for training the model.\n",
    "\n",
    "    save_weights : bool, optional\n",
    "        If True, the trained model is saved. By default, it's True.\n",
    "    \"\"\"\n",
    "    # Instantiate model\n",
    "    model = NESS(config)\n",
    "    # Start the clock to measure the training time\n",
    "    start = time.process_time()\n",
    "    # Fit the model to the data\n",
    "    model.fit(data_loader)\n",
    "    # Total time spent on training\n",
    "    training_time = time.process_time() - start\n",
    "    # Report the training time\n",
    "    print(\"Done with training...\")\n",
    "    print(f\"Training time:  {training_time//60} minutes, {training_time%60} seconds\")\n",
    "    # Return the best Test set AUC\n",
    "    return model.test_auc, model.test_ap, model.t_inference\n",
    "\n",
    "\n",
    "def main(config: Dict) -> None:\n",
    "    \"\"\"\n",
    "    The main function that starts the execution of the program. Takes the \n",
    "    configuration dictionary as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "    \"\"\"\n",
    "    # Ser directories (or create if they don't exist)\n",
    "    set_dirs(config)\n",
    "    # Get data loader for first dataset.\n",
    "    ds_loader = GraphLoader(config, dataset_name=config[\"dataset\"])\n",
    "    # Add the number of features in a dataset as the first dimension of the model\n",
    "    config = update_config_with_model_dims(ds_loader, config)\n",
    "    # Start training and save model weights at the end\n",
    "    test_auc, test_ap,t_inference = train(config, ds_loader, save_weights=True)\n",
    "    # Return best test auc\n",
    "    return test_auc, test_ap, t_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b140b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 54.765625 seconds\n",
      "Test AUC: 0.9400037718901485, AP: 0.9492822015242535, Time inference:0.08463191986083984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the main\n",
    "\n",
    "test_auc, test_ap, t_inference = main(config)\n",
    "\n",
    "print(f\"Test AUC: {test_auc}, AP: {test_ap}, Time inference:{t_inference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebf496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ae1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot utilities. Used to plot losses recorded during training.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def save_loss_plot(losses: Dict[str, List[float]], plots_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves loss plot. \n",
    "\n",
    "    If validation loss is present, the plot includes both training and validation loss; otherwise, it includes only\n",
    "    the training loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    losses : dict\n",
    "        A dictionary contains lists of losses. The keys are \"tloss_e\" for training loss and \"vloss_e\" for validation \n",
    "        loss. The values are lists of recorded loss values.\n",
    "    plots_path : str\n",
    "        Path to save the loss plot.\n",
    "    \"\"\"\n",
    "    x_axis = list(range(len(losses[\"tloss_e\"])))\n",
    "    plt.plot(x_axis, losses[\"tloss_e\"], c='r', label=\"Training\")\n",
    "    title = \"Training\"\n",
    "    if len(losses[\"vloss_e\"]) >= 1:\n",
    "        # If validation loss is recorded less often, we need to adjust x-axis values by the factor of difference\n",
    "        beta = len(losses[\"tloss_e\"]) / len(losses[\"vloss_e\"])\n",
    "        x_axis = list(range(len(losses[\"vloss_e\"])))\n",
    "        # Adjust the values of x-axis by beta factor\n",
    "        x_axis = [beta * i for i in x_axis]\n",
    "        plt.plot(x_axis, losses[\"vloss_e\"], c='b', label=\"Validation\")\n",
    "        title += \" and Validation \"\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(title + \" Loss\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(plots_path + \"/loss.png\")\n",
    "    #plt.clf()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def save_auc_plot(summary: Dict[str, List[float]], plots_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves AUC (Area Under the ROC Curve) plot during training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary : dict\n",
    "        A dictionary contains list of loss and AUC values stored during training. The key for AUC values is \n",
    "        \"val_auc\".\n",
    "    plots_path : str\n",
    "        Path to save the AUC plot.\n",
    "    \"\"\"\n",
    "    if len(summary[\"val_auc\"]) > 1:\n",
    "        \n",
    "        x_axis = list(range(len(summary[\"val_auc\"])))\n",
    "        plt.plot(x_axis, summary[\"val_auc\"], c='r', label=\"Validation\")\n",
    "            \n",
    "        title = \"AUCs during training\"\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"AUC\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path + \"/val_auc.png\")\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06091a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e351c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir='results/cora/training/loss/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccecebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.read_csv( Dir+'losses.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc2f480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAArpElEQVR4nO3deXxU5dn/8c9FQEASQRYRAQUtiFVki+K+24KlWnepVSltXZ9a7VNttYtWa22rfX60T6sVtW5VsVr1Uautu1KpS1BUUKygoCBiACGsksD1++M6A0PMnkxmyff9euWVmTMz51xzGOab+z73uY+5OyIiIrmmXbYLEBERqYkCSkREcpICSkREcpICSkREcpICSkREcpICSkREcpICSrLGzB4zszNa+rnZZGbzzOyIDKz3WTP7dnL7VDN7vCHPbcJ2djSzVWZW1NRaRVqKAkoaJfnySv1sNLO1afdPbcy63H2su9/W0s/NRWb2IzN7voblPc1svZnt0dB1ufud7v6lFqpri0B19w/cvdjdN7TE+uvY7gAzczNrn8ntSH5TQEmjJF9exe5eDHwAfDVt2Z2p5+mL53P+AuxnZgOrLT8FeNPdZ2ahJpGcpoCSFmFmh5jZAjP7oZl9DNxiZtua2SNmVm5mnya3+6W9Jr3baoKZ/cvMrk2e+76ZjW3icwea2fNmttLMnjSzP5rZX2qpuyE1XmlmLyTre9zMeqY9fpqZzTezpWb249r2j7svAJ4GTqv20OnA7fXVUa3mCWb2r7T7R5rZbDNbYWZ/ACztsV3M7OmkviVmdqeZdUseuwPYEXg4aQFfXL1lY2Y7mNlDZrbMzOaY2XfS1n25mf3VzG5P9s0sMyutbR80VD3b3NvMysyswswWm9n/JMs7mdlfkve53MxeMbPeza1FsksBJS1pe6A7sBNwJvH5uiW5vyOwFvhDHa8fDbwD9AR+A9xsZtaE594FvAz0AC7n86GQriE1fh34JrAdsBXwAwAz+yJwfbL+HZLt1RgqidvSazGzXYHhSb2N3VepdfQE7gd+QuyLucD+6U8Brk7q2w3oT+wT3P00tmwF/6aGTUwBFiSvPwH4pZkdlvb40clzugEPNaTmBqhrm78Dfufu2wC7AH9Nlp8BdE3eXw/gbGIfSh5TQElL2ghc5u6fuftad1/q7n9z9zXuvhK4Cji4jtfPd/cbk+MftwF9gNr+Cq7xuWa2I7AX8DN3X+/u/yK+OGvUwBpvcff/uPta4gtxeLL8BOARd3/e3T8Dfprsg9o8kNS4X3L/dOAxdy9vwr5KOQqY5e73uXslMAn4OO39zXH3J5J/k3Lgfxq4XsysPxF2P3T3de4+A7gpqTvlX+7+aPLvcAcwrCHrbsY2K4EvmFlPd1/l7i+mLe8BfMHdN7j7dHevaE4tkn0KKGlJ5e6+LnXHzLY2sxuSLrAK4Hmgm9U+Qiz9i3VNcrO4kc/dAViWtgzgw9oKbmCNH6fdXpNW0w7p63b31cDS2raV1HQvcHrS2jsVuL0RddSkeg2eft/MepvZFDNbmKz3L0RLqyFS+3Jl2rL5QN+0+9X3TSdr3vHH+rb5LWAwMDvpxhuXLL8D+Ccwxcw+MrPfmFmHZtQhOUABJS2p+tT4/w3sCoxOumQOSpbX1m3XEhYB3c1s67Rl/et4fnNqXJS+7mSbPep5zW3AScCRQAnwcDPrqF6DseX7/SXx7zI0We83qq2zrssZfETsy5K0ZTsCC+upqTnq3Ka7v+vu44nu1l8D95lZF3evdPefu/sXgf2AcWzZ0pM8pICSTCohjgMsN7PuwGWZ3qC7zwfKgMvNbCsz2xf4aoZqvA8YZ2YHmNlWwBXU/39qKrAcmAxMcff1zazj78DuZnZc0nI5nzgWmFICrAJWmFlf4KJqr18M7FzTit39Q2AacHUyCGFPogVT44CTJuqYrLuTmXUigqjWbZrZN8ysl7tvJPYjwEYzO9TMhiYtzgqiy6+u7lbJAwooyaRJQGdgCfAi8I9W2u6pwL5Ed9svgHuAz2p57iSaWKO7zwLOIwY5LAI+JQ7u1/UaJ7r1dkp+N6sOd18CnAj8ini/g4AX0p7yc2AksIIIs/urreJq4CfJyLcf1LCJ8cAAomXzAHGM8cmG1NZAq4hgTv0cVs82xwCzzGwVMWDilOTY4PbEHwwVwNvAc0S3n+Qx0wULpdCZ2T3AbHfPeAtORFqOWlBScMxsr+T8n3ZmNgY4Bngwy2WJSCPpbH8pRNsTXVk9iC63c9z9teyWJCKNpS4+ERHJSeriExGRnJRTXXw9e/b0AQMGZLsMERFpRdOnT1/i7r2qL8+pgBowYABlZWXZLkNERFqRmc2vaXlGu/jM7MJkhuOZZnZ3ciKeiIhIvTIWUMlZ6+cDpe6+B1BEXPtGRESkXpkeJNEe6JxMwbI1cWa4iIhIvTJ2DMrdF5rZtcT1ZtYCj7v749WfZ2ZnEtcOYscdd8xUOSIijVJZWcmCBQtYt25d/U+WBunUqRP9+vWjQ4eGTTSfsYAys22JM/gHEpM63mtm33D3LSaadPfJxMSZlJaW6qQsEckJCxYsoKSkhAEDBlD7dTOlodydpUuXsmDBAgYOHNig12Syi+8I4P3kYmyVxJn9+9XzGhGRnLBu3Tp69OihcGohZkaPHj0a1SLNZEB9AOyTXIjNgMOJWYZFRPKCwqllNXZ/Ziyg3P0lYvr7V4E3k21NztT2AHj0UfhHa13RQUREMimjo/jc/TJ3H+Lue7j7ae5e2zV5WsYvfwnXXpvRTYiItIalS5cyfPhwhg8fzvbbb0/fvn033V+/fn2dry0rK+P888+vdxv77ZfbR11yaiaJZmvfHqqqsl2FiEiz9ejRgxkzZgBw+eWXU1xczA9+sPmaklVVVbRvX/NXeGlpKaWlpfVuY9q0aS1Sa6YU1mSx7dtDZWW2qxARyYgJEyZw9tlnM3r0aC6++GJefvll9t13X0aMGMF+++3HO++8A8Czzz7LuHHjgAi3iRMncsghh7Dzzjvz+9//ftP6iouLNz3/kEMO4YQTTmDIkCGceuqppK508eijjzJkyBBGjRrF+eefv2m9rUEtKBGR+lxwASStmRYzfDhMmtToly1YsIBp06ZRVFRERUUFU6dOpX379jz55JNceuml/O1vf/vca2bPns0zzzzDypUr2XXXXTnnnHM+dy7Sa6+9xqxZs9hhhx3Yf//9eeGFFygtLeWss87i+eefZ+DAgYwfP76Jb7ZpCiugOnRQQIlIQTvxxBMpKioCYMWKFZxxxhm8++67mBmVtfQgfeUrX6Fjx4507NiR7bbbjsWLF9OvX78tnrP33ntvWjZ8+HDmzZtHcXExO++886bzlsaPH8/kyZkd65ausAJKLSgRyYQmtHQypUuXLptu//SnP+XQQw/lgQceYN68eRxyyCE1vqZjx46bbhcVFVFVw/dkQ57T2grvGFQO7FQRkdawYsUK+vbtC8Ctt97a4uvfddddee+995g3bx4A99xzT4tvoy4KKBGRPHXxxRdzySWXMGLEiIy0eDp37sx1113HmDFjGDVqFCUlJXTt2rXFt1MbS43UyAWlpaXerAsWnnYaTJsGc+e2XFEi0ia9/fbb7LbbbtkuI+tWrVpFcXEx7s55553HoEGDuPDCC5u8vpr2q5lNd/fPjYsvvBaUhpmLiLSYG2+8keHDh7P77ruzYsUKzjrrrFbbtgZJiIhIrS688MJmtZiao7BaUBpmLiJSMAoroNSCEhEpGAooERHJSQooERHJSYUXUBrFJyIF4NBDD+Wf//znFssmTZrEOeecU+PzDznkEFKn6Rx11FEsX778c8+5/PLLubaeSxI9+OCDvPXWW5vu/+xnP+PJJ59sZPUto/ACqqoKcujcLhGRphg/fjxTpkzZYtmUKVMaNGHro48+Srdu3Zq03eoBdcUVV3DEEUc0aV3NVVgBlZqdd+PG7NYhItJMJ5xwAn//+983XZxw3rx5fPTRR9x9992Ulpay++67c9lll9X42gEDBrBkyRIArrrqKgYPHswBBxyw6XIcEOc37bXXXgwbNozjjz+eNWvWMG3aNB566CEuuugihg8fzty5c5kwYQL33XcfAE899RQjRoxg6NChTJw4kc8++2zT9i677DJGjhzJ0KFDmT17dovsg8I7DwqiFZXM9isi0lzZuNpG9+7d2XvvvXnsscc45phjmDJlCieddBKXXnop3bt3Z8OGDRx++OG88cYb7LnnnjWuY/r06UyZMoUZM2ZQVVXFyJEjGTVqFADHHXcc3/nOdwD4yU9+ws0338x3v/tdjj76aMaNG8cJJ5ywxbrWrVvHhAkTeOqppxg8eDCnn346119/PRdccAEAPXv25NVXX+W6667j2muv5aabbmruLiqwFlR6QImI5Ln0br5U995f//pXRo4cyYgRI5g1a9YW3XHVTZ06lWOPPZatt96abbbZhqOPPnrTYzNnzuTAAw9k6NCh3HnnncyaNavOWt555x0GDhzI4MGDATjjjDN4/vnnNz1+3HHHATBq1KhNk8s2V+G2oEREWki2rrZxzDHHcOGFF/Lqq6+yZs0aunfvzrXXXssrr7zCtttuy4QJE1i3bl2T1j1hwgQefPBBhg0bxq233sqzzz7brFpTl+toyUt1qAUlIpKjiouLOfTQQ5k4cSLjx4+noqKCLl260LVrVxYvXsxjjz1W5+sPOuggHnzwQdauXcvKlSt5+OGHNz22cuVK+vTpQ2VlJXfeeeem5SUlJaxcufJz69p1112ZN28ec+bMAeCOO+7g4IMPbqF3WrPCDCgNNReRAjF+/Hhef/11xo8fz7BhwxgxYgRDhgzh61//Ovvvv3+drx05ciQnn3wyw4YNY+zYsey1116bHrvyyisZPXo0+++/P0OGDNm0/JRTTuGaa65hxIgRzE27MkSnTp245ZZbOPHEExk6dCjt2rXj7LPPbvk3nKawLrdx441w5pnw4YdQ7XLGIiKNocttZEbbvdxGapi5uvhERPJeYQWUjkGJiBQMBZSISC1y6RBIIWjs/sxYQJnZrmY2I+2nwswuyNT2AAWUiLSYTp06sXTpUoVUC3F3li5dSqdOnRr8moydB+Xu7wDDAcysCFgIPJCp7QEaxSciLaZfv34sWLCA8vLybJdSMDp16kS/Rgxga60TdQ8H5rr7/IxuRS0oEWkhHTp0YODAgdkuo01rrWNQpwB31/SAmZ1pZmVmVtbsv1QUUCIiBSPjAWVmWwFHA/fW9Li7T3b3Uncv7dWrV/M2pmHmIiIFozVaUGOBV919cca3pBaUiEjBaI2AGk8t3XstTgElIlIwMhpQZtYFOBK4P5Pb2UQBJSJSMDI6is/dVwM9MrmNLWiYuYhIwdBMEiIikpMUUCIikpMKK6A0zFxEpGAUVkCpBSUiUjAUUCIikpMUUCIikpMKM6A0zFxEJO8VZkCpBSUikvcUUCIikpMKK6A0zFxEpGAUVkCpBSUiUjAKK6CKiuK3AkpEJO8VVkC1axc/GsUnIpL3CiugILr51IISEcl7CigREclJhRdQHToooERECkDhBZRaUCIiBUEBJSIiOUkBJSIiOakwA0rDzEVE8l5hBpRaUCIieU8BJSIiOanwAkrDzEVECkLhBZRaUCIiBUEBJSIiOSmjAWVm3czsPjObbWZvm9m+mdweoIASESkQ7TO8/t8B/3D3E8xsK2DrDG9Pw8xFRApExgLKzLoCBwETANx9PbA+U9vbRC0oEZGCkMkuvoFAOXCLmb1mZjeZWZcMbi8ooERECkImA6o9MBK43t1HAKuBH1V/kpmdaWZlZlZWXl7e/K1qmLmISEHIZEAtABa4+0vJ/fuIwNqCu09291J3L+3Vq1fzt6oWlIhIQchYQLn7x8CHZrZrsuhw4K1MbW8TBZSISEHI9Ci+7wJ3JiP43gO+meHtaRSfiEiByGhAufsMoDST2/gctaBERAqCZpIQEZGcpIASEZGcVHgBlT7MvKxMx6NERPJU4QVUqgVVXg6jR8Nvf5vtikREpAkKN6CWLYONG+G228A921WJiEgjFWZAVVbC6tVxf/ZsmD49uzWJiEijFWZAVVXBqlWbl/3lL9mrR0REmqTwA2qnneDuuzWyT0QkzxReQHXoABs2bA6oiRPhk09iRJ+IiOSNwguo9snkGCtWxO/ddovfy5Zlpx4REWmSwg2o5cvj9w47xO+KiqyUIyIiTVO4AfXpp/G7b9/4nWpRiYhIXijcgFq+HIqKoGfPuK8WlIhIXincgFqxAoqLoUuXCCq1oERE8krhBtTy5RFQZrDNNmpBiYjkmcILqA4d4ncqoCACSi0oEZG8UngBld6C6tIlbnftqhaUiEieKdyASh2DAnXxiYjkocINqE8/VRefiEgeK9yAWrNGXXwiInmscAMK1IISEcljbSeg1IISEckrDQooM+tiZu2S24PN7Ggz65DZ0pqoQ1pZqYDq2hXWrYP167NTk4iINFpDW1DPA53MrC/wOHAacGumimqW9BZU6hjUNtvEb7WiRETyRkMDytx9DXAccJ27nwjsnrmymqGmLr6uXeO3AkpEJG80OKDMbF/gVODvybKizJTUTLUdgwINlBARySPt638KABcAlwAPuPssM9sZeKa+F5nZPGAlsAGocvfSJtbZcHUFlFpQIiJ5o0EB5e7PAc8BJIMllrj7+Q3cxqHuvqSJ9TVeTcegUl18akGJiOSNho7iu8vMtjGzLsBM4C0zuyizpTWRWlAiIgWhocegvujuFcDXgMeAgcRIvvo48LiZTTezM2t6gpmdaWZlZlZWXl7ewHLqUNMwcwWUiEjeaWhAdUjOe/oa8JC7VxLhU58D3H0kMBY4z8wOqv4Ed5/s7qXuXtqrV6+G1l07dfGJiBSEhgbUDcA8oAvwvJntBNTbHHH3hcnvT4AHgL2bVmYj1NTF17FjtKzUghIRyRsNCih3/72793X3ozzMBw6t6zXJ7BMlqdvAl4jjV5lVU0CZRStKLSgRkbzRoFF8ZtYVuAxIddE9B1wB1PWN3xt4wMxS27nL3f/R9FIbqKYuPtB8fCIieaah50H9mWj9nJTcPw24hZhZokbu/h4wrFnVNUUqoDp0gK222rxcASUiklcaGlC7uPvxafd/bmYzMlBP86UCKtW9l6IuPhGRvNLQQRJrzeyA1B0z2x9Ym5mSmik1zLx6QKkFJSKSVxragjobuD05FgXwKXBGZkpqplQLKv34E+iihSIieaahUx29Dgwzs22S+xVmdgHwRgZra5q6uvjUghIRyRuNuqKuu1ckM0oAfD8D9TRfUTLJek1dfCtWgDfk/GIREcm25lzy3VqsipZkFiFVU0BVVcWVdUVEJOc1J6BytynSvn3Nx6BA3XwiInmizmNQZraSmoPIgM4ZqagldOjw+RZU6v6qVdC7d+vXJCIijVJnQLl7SWsV0qL69oWBA7dcVpK8lVWrWr8eERFptIYOM88vZWUxQWy69BaUiIjkvMIMqOrde+nLVq5s3VpERKRJmjNIIr+oi09EJK+0nYBSF5+ISF5RQImISE5qewGlY1AiInmh7QRUp04xw4RaUCIieaHtBJRZtKIUUCIieaHtBBREQKmLT0QkL7StgCopUQtKRCRPtK2AUhefiEjeaHsBpS4+EZG80PYCSi0oEZG80LYCSsegRETyRtsKKLWgRETyRtsLKB2DEhHJC20roEpKYPVq2Lgx25WIiEg9Mh5QZlZkZq+Z2SOZ3la9UvPxrVmT3TpERKRerdGC+h7wditsp36aMFZEJG9kNKDMrB/wFeCmTG6nwXTJDRGRvJHpFtQk4GKg1oM+ZnammZWZWVl5eXlmq9FVdUVE8kbGAsrMxgGfuPv0up7n7pPdvdTdS3v16pWpcoK6+ERE8kYmW1D7A0eb2TxgCnCYmf0lg9urn7r4RETyRsYCyt0vcfd+7j4AOAV42t2/kantNYi6+ERE8kbbOg9KXXwiInmjfWtsxN2fBZ5tjW3VSV18IiJ5o221oNTFJyKSN9pWQG21FXTooIASEckDbSugQBPGiojkibYZUGpBiYjkvLYXULpooYhIXmh7AaUuPhGRvNA2A0otKBGRnNf2AkpdfCIieaHtBZS6+ERE8kLbC6hu3WDZsmxXISIi9Wh7AbXDDrBihS77LiKS49pmQAEsWpTdOkREpE5tL6D69o3fCxdmtw4REalT2wuoVAvqo4+yW4eIiNRJASUiIjmp7QVUt27QubO6+EREclzbCyizaEVVb0FVVWWnHhERqVHbCyiIgRLpLaiFC6FXLzj7bAWViEiOaJsBVb0FdfvtsHw53HADHHMMrFuXtdJERCS0zYDq2zcCyj1+brsNDjoIrr8eHn0Urr462xWKiLR5bTOgdtgB1q6NVtNLL8E778AZZ0QX36mnwq9+FctERCRr2m5AQbSibr0Vtt4aTjwxlv32tzHK79xzo3UlIiJZ0TYDKjWbxLvvwpQpcNxxcRkOgN694Yor4Omn4cUXs1ejiEgb1zYDKtWCuvbamDj2rLO2fPyb34QuXeCmm1q/NhERAdp6QL3wAgwdCvvvv+XjJSVwyilwzz26dpSISJZkLKDMrJOZvWxmr5vZLDP7eaa21WidO8O228btc8+Nk3er+/a3YfXqCCkREWl1mWxBfQYc5u7DgOHAGDPbJ4Pba5y+faOldOqpNT8+ejTsvru6+UREsqR9plbs7g6sSu52SH5yZ1jcBRdAu3abB0dUZwYTJsBFF8GcOfCFL7RmdSIibV5Gj0GZWZGZzQA+AZ5w95dqeM6ZZlZmZmXl5eWZLGdL3/pWDIaoy8knx29184mItLqMBpS7b3D34UA/YG8z26OG50x291J3L+3Vq1cmy2m8/v3hgAPg7ruzXYmISJvTKqP43H058AwwpjW216LGj4dZs2DmzGxXIiLSpmRyFF8vM+uW3O4MHAnMztT2MuaEE6CoCO66K9uViIi0KZlsQfUBnjGzN4BXiGNQj2Rwe5mx3XYwdiz86U+wbFm2qxERaTMyFlDu/oa7j3D3Pd19D3e/IlPbyrirroqJZX/xi2xXIiLSZrTNmSQaa889Y9TfH/4Q8/elaDJZEZGMUUA11JVXQseOcNJJ0dX3pz9Bjx7wy1/qKrwiIhmggGqo7beH++6Dt96KFtU550DXrvDjH8O++8byysq4fMfkyTEJrYiINJkCqjG+/OUIqfLymKvv3Xfhr3+FefNgxAgYMiRO/j3rrJiQ9oYbsl2xiEjeUkA11le/GgMmbrwR2rePCx3OmgVf+xoUF8P//R+88krMkH7uufDYY9muWEQkL5nn0IH+0tJSLysry3YZLWP16piFYu5cuP9+OOKIbFckIpKTzGy6u5dWX64WVKZ06QIPPRTnUR15JBx7LLz0kkb+iYg0kAIqk/r3jymSrroKnnoK9tknBlS88062KxMRyXnq4mstK1fCHXfAZZfBunXw3e9CRQX06ROzputyHiLSRtXWxaeAam0LFsQEtP/6VwxTTw1H32uvWH7mmdE9KCLSRugYVK7o1w+efx7WrInRgB98ANdcAxs3wve/D2PGRGtLRKSNU0Blgxl07hy3+/eHH/wAysriwoj//neM/jv++JhJ/YUXsluriEiWZOyS79IEJ50EHTpES2r27Dgh+G9/i8EVo0bFeVfz5kHfvnDwwXHu1VZbZbtqEZGM0DGoXLZmDfzxj/DAAzEacONGGDAA5s+HVaviMiD33w+PPgrPPAO/+pWOX4lI3tEgiXyX+ncyi8lpb7wRzjsvplRauDAeO/xweOQR6NRpy9cuXBgXXezeXS0uEck5GiSR78ziB6Kr75xz4JZb4LPP4Oqr4eab41yrI4+EZ5+F11+PIe177BEDM/r0iamYzj4b3n47BmfcfjvsvXfMMfjGG1l9eyIi1akFVUhuuy0GXCxZEvfbtYMDD4Sjj45BGTNmRKhVVm5+ze67w8cfw6efwle+AoccEgM1pk2L415jx8LXvx7Hw849Ny4xMnkylJTE69esiaHzgwZtDlARkUZQF19bsXYtTJkCGzZEMG233ZaPf/ABPPFE3B4wAA47LMLpV7+Kmdnnz4/XHHwwvPpqzCVYXBzHv9q3jzkGhwyJyXBffhnefDO2tcsucVHH889v/HGwysoI06KiFtkFIpJfFFBSP3f48MM4rtU+GeBZVgbXXRchcvXVMU3T+PFxf++942eHHeDee2OgRv/+MQqxX79ogQ0cGOtZtgy23vrzx8emTo0WWs+ecfysuBgefhgOPTRGK1ZWxoCQbbdt3X0hIq1GASUtZ8OGaPFU79KbOjWmcHr99c3LDjooWmhvvhn3+/SJS5aMGhXPv+uuaMl98glss01MA7VsWdz+znci+BYsgB/9KI6ppQZ5bNwI//xnPN6rV6zv2GNjmP7q1bG9JUtg112j+zHd+vUaLCKSQxRQ0jrcYdEiWLwY/v53uPvu6DL80pci2GbOjJbS6tVxPOuEE2Imjblz4bjjIlD+67/gt7+NFtk++0Qr7O674zhax44RjlVVMZdh165xHKyyMq50fPzx8L//u/k4HERX50UXwejRcPHF8Ic/xPG0yy+PoNtqqzimtmFDHH8rKYljcytWxGwfO+9c//E196ijKcP8V62KYO7Zc/Oyqip48MEYmanWoxQ4BZTkjjVroitx0KAIm5q4w0cfRfehWZzr9cQTESIbN8bjqRk3zKJb8Hvfi9bWkUfGEPzttosLRv7xj9Eq69UrBnscfHBMN5X67LdrByNHxnD8RYs2L9u4MW6nRjo+/XQE1kEHxWOvvRZdltttF5dSWbAgQnbMmLhw5Zw50K1b/HTvDhMnRuC4w/vvRzfoyy9Hl+nq1fD441BaGoF18snxnvv0gf/3/2JfVVTENrfdFk49NVqLtXGvPVTdYy7IadOie7Vv32iJbtwYtTTW+vVx7LKiIvajBstIIymgpPCtWhWhNnjwlstXr4Zbb4U774wuyPHjYfr0aMltu220tp57LkJk/PhovcycGYEGcQxuzpzoRuzZM6afKiqKL+PKytjm8OFx/O2WW+KLun//CLaKigi1+fMjHM8+O1ppM2ZEwGzYEC3EjRujjlNOiSB8/3346U/jROxU92i6IUNg3LgI+wEDorYePSJkf/3rCL4RI6LVOHp01PHCCxHAH3yw+ZIvHTvCTjvBf/4T9y+5JEaC/vvfsHRp7IsBA+I55eWxj/v0iff95puxDx95JOoAuOCCCNRPPol9PGpUBLh7vH7BggjE3r1r/jdctiz29cCBm/d/Qy1ZEqdL7Lvv5qnEqquqiinFPvoo3vuECdGd3BxVVfF5aEowu8e/VVlZ/FGS3opuioqK+KMpz7qwFVAiTbVhQ3wxd+0a91Mtq5pafytWRLjsueeWj69ZE+H45z/HpVXOPTe+xN3h0kvji2XcuHjtyJHwwx9GS2z9+ujqXLcuvnSHDYvW2iWXxHM7dYpjfOn69o1uzTffjC++detiee/eETY9esTxugMOiO7VWbNi9OVzz8UpBI3Rq1e0YvfZJ44p3nxzrOvuuyOQILpMV6/evN8AdtwxQm/77eNn7do4j+/99+PxrbeO/VVUFO+3Q4f4A2KXXWLfjhkTA2o+/DCmA7vrrniv7psH6qxbF+t7770Io333jXBKD/wDD4yZWi65JFqwX/5y/LGxfHmEXVlZ/HsdcUT8G378cXweiovj3+LNN+H666OlP2lSnJaRrqoq/p3XrYs/htK7a+fMiT+IUt95JSXRvX3SSRHqZWWxb/baa8vwq6yEn/0sWsCTJkW9ZWXwu9/FCF6IP9LOOy+O49YUVhs2xB8pAwfWHeZTp0br+tln4ZvfjD9eMtBCVkCJ5ILZs+MLLzVKMl36bCGNUV4eX6YrVsR6v/zl+EKG+DKbOTOOjdV3rpp7BMyiRdGN2a9fhOzcuREEvXvHej76KJbvsUccM0y9l6oqOOqo6IodNgyuvBLeeiu+1Lt0iS/dvn1jPslXX43W3scfxw/EOXgHHBDH/O69N75si4riC7hdu2ghzZ8fQdepU3zZp7pkR46MuSkHD4bf/CbWD9EiGTgw9s1//hPhNWlS7KOHH46u0k6dInwOPxxefDHCFCJ09tor3sO778ay7t3j/VZUROi0axcBPWNGPGfQoNh3S5ZEAMydu+V5h7vtFnWOHRvhtG5d7KfS0hgl++CDn7/q9k47wYknxnmKn3wSx1CnTo2W3+rV8e80f36E5sSJEXTPPBMBttNOcPrpsd0nn4x9v3FjfF6WLYv98+1vw377xb/ve+/FZ3T27GjJl5fHHwuDBsXgp3PPjUFOEJ+l9u1jvzWTAkpEMq+iAh56KAa/VD+loLEWLIhWa+qkcIgW5UsvwX33Rctxr72idbPbbpufs3FjfNH27r3la5csifup8Ibo+v3FLyK0xo3bPIq0W7f4Yk755JOoJfXaqqoIoJKSaA2uXx+t44ceipDbfvvohh0yJB7v3DkC+bnnIig2boyW7NNPR4swJTW4aNWqCK13343zE594YnPQFRfDDTdEyF16afzxcOyxcfwz1UJzj1Gu11wTYeUej33xi/H4oEHRonzssThemp4DZhHqe+8d/45jxkT9//3fsZ/Sde0arc1mavWAMrP+wO1Ab8CBye7+u7peo4ASkYL3/vtxde3jj4/Rog3x6acxsKV//wi9xoT/okUR9iNG1Nxy//TTaDEtXhxdqIMG1bx+92hNrly5OdCKiiLImikbAdUH6OPur5pZCTAd+Jq7v1XbaxRQIiJtT6tPFuvui9z91eT2SuBtoG+mticiIoWlVWYzN7MBwAjgpRoeO9PMysysrDw16kdERNq8jAeUmRUDfwMucPeK6o+7+2R3L3X30l6NPe9BREQKVkYDysw6EOF0p7vfn8ltiYhIYclYQJmZATcDb7v7/2RqOyIiUpgy2YLaHzgNOMzMZiQ/R2VweyIiUkBqGBTfMtz9X4BmjRQRkSZplVF8IiIijZVTUx2ZWTkwv5mr6QksqfdZuUd1t558rBlUd2tT3a1nJ3f/3DDunAqolmBmZTWdkZzrVHfryceaQXW3NtWdferiExGRnKSAEhGRnFSIAdXIK67lDNXdevKxZlDdrU11Z1nBHYMSEZHCUIgtKBERKQAKKBERyUkFE1BmNsbM3jGzOWb2o2zXUxsz629mz5jZW2Y2y8y+lyy/3MwW5vK0UGY2z8zeTOorS5Z1N7MnzOzd5Pe22a4znZntmrZPZ5hZhZldkIv728z+bGafmNnMtGU17l8Lv08+72+Y2cgcq/saM5ud1PaAmXVLlg8ws7Vp+/1POVZ3rZ8LM7sk2d/vmNmXc6jme9LqnWdmM5LlObOvm8zd8/4HKALmAjsDWwGvA1/Mdl211NoHGJncLgH+A3wRuBz4Qbbrq6f2eUDPast+A/wouf0j4NfZrrOez8nHwE65uL+Bg4CRwMz69i9wFPAYMZ3YPsBLOVb3l4D2ye1fp9U9IP15Obi/a/xcJP9HXwc6AgOT75uiXKi52uO/BX6Wa/u6qT+F0oLaG5jj7u+5+3pgCnBMlmuqkRfelYaPAW5Lbt8GfC17pdTrcGCuuzd3tpKMcPfngWXVFte2f48BbvfwItDNzPq0SqHV1FS3uz/u7lXJ3ReBfq1eWD1q2d+1OQaY4u6fufv7wBzie6dV1VVzcgWJk4C7W7WoDCqUgOoLfJh2fwF58KVfw5WG/yvpEvlzrnWVJRx43Mymm9mZybLe7r4ouf0x0Ds7pTXIKWz5nzfX9zfUvn/z6TM/kWjtpQw0s9fM7DkzOzBbRdWhps9FPuzvA4HF7v5u2rJc39d1KpSAyjv2+SsNXw/sAgwHFhFN9VxzgLuPBMYC55nZQekPevQr5OR5C2a2FXA0cG+yKB/29xZyef/Wxsx+DFQBdyaLFgE7uvsI4PvAXWa2Tbbqq0HefS7SjGfLP8ByfV/Xq1ACaiHQP+1+v2RZTrIarjTs7ovdfYO7bwRuJAvdB/Vx94XJ70+AB4gaF6e6lpLfn2SvwjqNBV5198WQH/s7Udv+zfnPvJlNAMYBpybhStJFtjS5PZ04ljM4a0VWU8fnIqf3t5m1B44D7kkty/V93RCFElCvAIPMbGDyl/IpwENZrqlGST/x5640XO34wbHAzOqvzSYz62JmJanbxEHwmcR+PiN52hnA/2Wnwnpt8ddlru/vNLXt34eA05PRfPsAK9K6ArPOzMYAFwNHu/uatOW9zKwoub0zMAh4LztVfl4dn4uHgFPMrKOZDSTqfrm166vDEcBsd1+QWpDr+7pBsj1Ko6V+iFFN/yH+Svhxtuupo84DiG6aN4AZyc9RwB3Am8nyh4A+2a61Wt07E6OYXgdmpfYx0AN4CngXeBLonu1aa6i9C7AU6Jq2LOf2NxGgi4BK4hjHt2rbv8TovT8mn/c3gdIcq3sOccwm9Rn/U/Lc45PPzwzgVeCrOVZ3rZ8L4MfJ/n4HGJsrNSfLbwXOrvbcnNnXTf3RVEciIpKTCqWLT0RECowCSkREcpICSkREcpICSkREcpICSkREcpICSqQFmdmGarOnt9jM+sns1Ll6vpZIi2uf7QJECsxadx+e7SJECoFaUCKtILlOz28srqf1spl9IVk+wMyeTiYnfcrMdkyW906uo/R68rNfsqoiM7vR4lpij5tZ56y9KZEMU0CJtKzO1br4Tk57bIW7DwX+AExKlv0vcJu770lMqPr7ZPnvgefcfRhx/Z9ZyfJBwB/dfXdgOTFbgEhB0kwSIi3IzFa5e3ENy+cBh7n7e8lkwR+7ew8zW0JMp1OZLF/k7j3NrBzo5+6fpa1jAPCEuw9K7v8Q6ODuv2iFtybS6tSCEmk9Xsvtxvgs7fYGdBxZCpgCSqT1nJz2+9/J7WnE7PsApwJTk9tPAecAmFmRmXVtrSJFcoX++hJpWZ3NbEba/X+4e2qo+bZm9gbRChqfLPsucIuZXQSUA99Mln8PmGxm3yJaSucQs1iLtBk6BiXSCpJjUKXuviTbtYjkC3XxiYhITlILSkREcpJaUCIikpMUUCIikpMUUCIikpMUUCIikpMUUCIikpP+Pzlt1m/xqS+kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_plot(losses, Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2c4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4c55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669318e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18fc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAEWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22918223",
   "metadata": {},
   "source": [
    "##  Measurement of avarage of auc, and time inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12da9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics as stat\n",
    "\n",
    "\n",
    "Eva_final=dict()\n",
    "\n",
    "\n",
    "Base_model_Auc=[]\n",
    "T_base_model=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375b2702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.765625 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.05338859558105469\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.765625 seconds\n",
      "Test AUC: 0.9400037718901485, AP: 0.9492822015242535, Time inference:0.06249117851257324\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 50.890625 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.06904840469360352\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.359375 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.06248283386230469\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 55.703125 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.06249856948852539\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.734375 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.053392887115478516\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.1875 seconds\n",
      "Test AUC: 0.9400045710194173, AP: 0.9492830638084249, Time inference:0.05339694023132324\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 49.484375 seconds\n",
      "Test AUC: 0.9400029727608797, AP: 0.9492822015242536, Time inference:0.046889305114746094\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 48.765625 seconds\n",
      "Test AUC: 0.9400037718901485, AP: 0.9492822015242535, Time inference:0.05341219902038574\n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3542, X recon loss:0.9532, val auc:0.8980034408477786\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2049, X recon loss:0.8895, val auc:0.917390738625685\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1607, X recon loss:0.8773, val auc:0.9202099206291836\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1226, X recon loss:0.8563, val auc:0.9204412381269066\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  2.0 minutes, 48.78125 seconds\n",
      "Test AUC: 0.9400045710194173, AP: 0.9492830638084249, Time inference:0.05340003967285156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    \n",
    "        for epoch in range(60):\n",
    "        train()\n",
    "        train_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n",
    "        test_auc, test_ap, t_inference = main(config)\n",
    "\n",
    "        print(f\"Test AUC: {test_auc}, AP: {test_ap}, Time inference:{t_inference}\")\n",
    "       \n",
    "\n",
    "        Base_model_Auc.append(test_auc)\n",
    "        T_base_model.append(t_inference)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2b12f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9400029727608797,\n",
       " 0.9400037718901485,\n",
       " 0.9400029727608797,\n",
       " 0.9400029727608797,\n",
       " 0.9400029727608797,\n",
       " 0.9400029727608797,\n",
       " 0.9400045710194173,\n",
       " 0.9400029727608797,\n",
       " 0.9400037718901485,\n",
       " 0.9400045710194173]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_model_Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d657586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05338859558105469,\n",
       " 0.06249117851257324,\n",
       " 0.06904840469360352,\n",
       " 0.06248283386230469,\n",
       " 0.06249856948852539,\n",
       " 0.053392887115478516,\n",
       " 0.05339694023132324,\n",
       " 0.046889305114746094,\n",
       " 0.05341219902038574,\n",
       " 0.05340003967285156]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b5c0885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base_model_Auc in ten time:\n",
      "0.9400029727608797|0.9400037718901485|0.9400029727608797|0.9400029727608797|0.9400029727608797|0.9400029727608797|0.9400045710194173|0.9400029727608797|0.9400037718901485|0.9400045710194173  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Time Inference model in ten time:\n",
      "0.05338859558105469|0.06249117851257324|0.06904840469360352|0.06248283386230469|0.06249856948852539|0.053392887115478516|0.05339694023132324|0.046889305114746094|0.05341219902038574|0.05340003967285156 \n"
     ]
    }
   ],
   "source": [
    "print(f'Base_model_Auc in ten time:')\n",
    "print (\"{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<15}|{:<20}\"\\\n",
    "    .format(Base_model_Auc[0],Base_model_Auc[1],Base_model_Auc[2],\\\n",
    "        Base_model_Auc[3],Base_model_Auc[4],Base_model_Auc[5],\\\n",
    "        Base_model_Auc[6],Base_model_Auc[7],Base_model_Auc[8],Base_model_Auc[9]))\n",
    "print (\"-\"*110)\n",
    "print(f'Time Inference model in ten time:')\n",
    "#print(T_base_model)\n",
    "print (\"{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<10}|{:<15}|{:<20}\"\\\n",
    "    .format(T_base_model[0],T_base_model[1],T_base_model[2],\\\n",
    "        T_base_model[3],T_base_model[4],T_base_model[5],\\\n",
    "        T_base_model[6],T_base_model[7],T_base_model[8],T_base_model[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb0e589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc: 0.940 ± 0.000\n",
      "Time inference :0.057 ± 0.007\n"
     ]
    }
   ],
   "source": [
    "import statistics as stat\n",
    "\n",
    "base_model_accuracy_mean = stat.mean(Base_model_Auc)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_Auc)\n",
    "desc_auc = \"{:.3f} ± {:.3f}\".format(base_model_accuracy_mean,base_model_accuracy_std)\n",
    "print(f\"Auc: {desc_auc}\"  )\n",
    "\n",
    "Eva_final.update({'base model accuracy':float(format(base_model_accuracy_mean, '.4f'))})\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)   \n",
    "desc_T = \"{:.3f} ± {:.3f}\".format(t_base_model_mean,t_base_model_std)\n",
    "\n",
    "print(f\"Time inference :{desc_T}\")\n",
    "Eva_final.update({'time inference of base model':float(format(t_base_model_mean, '.6f'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3725b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
